# Booking.com Scrapers

Ce projet contient plusieurs scrapers pour diffÃ©rents objectifs liÃ©s Ã  Booking.com.

## ğŸ“ Structure du projet

```
booking/
â”œâ”€â”€ scrapers/                    # Tous les scrapers organisÃ©s par type
â”‚   â”œâ”€â”€ legal-info/             # Scraper pour informations lÃ©gales
â”‚   â”‚   â”œâ”€â”€ scraper3.js         # Scraper principal
â”‚   â”‚   â”œâ”€â”€ scraper-with-cron.js # Version avec cron jobs
â”‚   â”‚   â””â”€â”€ start-cron-scraper.sh
â”‚   â”œâ”€â”€ arrondissement-count/   # Scraper pour compter propriÃ©tÃ©s par arrondissement
â”‚   â”‚   â”œâ”€â”€ arrondissement-scraper.js
â”‚   â”‚   â””â”€â”€ start-arrondissement-scraper.sh
â”‚   â””â”€â”€ hotel-rooms/            # Scraper pour types de chambres (Ã  venir)
â”œâ”€â”€ config/                     # Fichiers de configuration
â”‚   â””â”€â”€ bigquery-credentials.json.example
â”œâ”€â”€ data/                       # DonnÃ©es et fichiers de sortie
â”‚   â”œâ”€â”€ hotel_room_types.csv
â”‚   â”œâ”€â”€ url.txt
â”‚   â””â”€â”€ legal-results-*.csv
â”œâ”€â”€ cloud-function/             # Version Cloud Function (pour dÃ©ploiement)
â””â”€â”€ package.json
```

## ğŸ¯ Types de scrapers

### 1. **Legal Info Scraper** (`scrapers/legal-info/`)
**Objectif** : Extraire les informations lÃ©gales des hÃ´tels (email, tÃ©lÃ©phone, nom entreprise, etc.)

**FonctionnalitÃ©s** :
- âœ… Scraping des informations lÃ©gales
- âœ… Cron jobs automatiques (quotidien Ã  8h00)
- âœ… Sauvegarde CSV/JSON
- âœ… Gestion d'erreurs robuste

**Utilisation** :
```bash
cd scrapers/legal-info
./start-cron-scraper.sh
```

### 2. **Arrondissement Count Scraper** (`scrapers/arrondissement-count/`)
**Objectif** : Compter le nombre de propriÃ©tÃ©s hÃ´teliÃ¨res par arrondissement de Paris

**FonctionnalitÃ©s** :
- âœ… Scraping des 20 arrondissements de Paris
- âœ… IntÃ©gration BigQuery (projet oversight-datalake)
- âœ… Comptage automatique des propriÃ©tÃ©s
- âœ… Insertion en base de donnÃ©es

**Utilisation** :
```bash
cd scrapers/arrondissement-count
./start-arrondissement-scraper.sh
```

**PrÃ©requis** :
- Credentials BigQuery configurÃ©s
- Variable `GOOGLE_APPLICATION_CREDENTIALS` dÃ©finie

### 3. **Hotel Rooms Scraper** (`scrapers/hotel-rooms/`)
**Objectif** : Scraper les types de chambres et prix (Ã  dÃ©velopper)

## ğŸ”§ Configuration

### BigQuery Credentials
1. Copiez `config/bigquery-credentials.json.example` vers `config/bigquery-credentials.json`
2. Remplissez avec vos vraies credentials
3. Ou dÃ©finissez la variable d'environnement :
   ```bash
   export GOOGLE_APPLICATION_CREDENTIALS=/path/to/your/credentials.json
   ```

### DÃ©pendances
```bash
npm install
```

## ğŸš€ DÃ©marrage rapide

### Scraper Legal Info (avec cron jobs)
```bash
cd scrapers/legal-info
./start-cron-scraper.sh
```

### Scraper Arrondissement (BigQuery)
```bash
cd scrapers/arrondissement-count
./start-arrondissement-scraper.sh
```

## ğŸ“Š BigQuery

### Dataset : `MarketData`
### Table : `ArrondissementSummary`

**SchÃ©ma** :
- `ObservationDate` (DATETIME) : Date/heure de l'observation
- `Arrondissement` (STRING) : NumÃ©ro de l'arrondissement (1-20)
- `PropertiesCount` (INTEGER) : Nombre de propriÃ©tÃ©s trouvÃ©es

### RequÃªtes utiles
```sql
-- Voir les derniÃ¨res donnÃ©es
SELECT * FROM `oversight-datalake.MarketData.ArrondissementSummary` 
ORDER BY ObservationDate DESC 
LIMIT 20;

-- Compter les propriÃ©tÃ©s par arrondissement (derniÃ¨re observation)
SELECT 
  Arrondissement,
  PropertiesCount,
  ObservationDate
FROM `oversight-datalake.MarketData.ArrondissementSummary` 
WHERE ObservationDate = (
  SELECT MAX(ObservationDate) 
  FROM `oversight-datalake.MarketData.ArrondissementSummary`
)
ORDER BY CAST(Arrondissement AS INT64);
```

## ğŸ”„ Cron Jobs

### Local (node-cron)
- **Legal Info** : Tous les jours Ã  8h00 (heure de Paris)
- **Arrondissement** : Manuel ou via GitHub Actions

### Cloud (Google Cloud Scheduler)
- Configuration disponible dans `cloud-function/`
- NÃ©cessite un projet Google Cloud avec facturation

## ğŸ“ Logs et monitoring

### Logs locaux
- Tous les scrapers affichent des logs dÃ©taillÃ©s
- Fichiers de sortie dans `data/`

### BigQuery
- DonnÃ©es historiques stockÃ©es automatiquement
- RequÃªtes de monitoring disponibles

## ğŸ› ï¸ DÃ©veloppement

### Ajouter un nouveau scraper
1. CrÃ©er un dossier dans `scrapers/`
2. Ajouter le script principal
3. CrÃ©er un script de dÃ©marrage
4. Documenter dans ce README

### Tests
```bash
# Test legal info
cd scrapers/legal-info
node scraper-with-cron.js test

# Test arrondissement (nÃ©cessite credentials BigQuery)
cd scrapers/arrondissement-count
node arrondissement-scraper.js
```