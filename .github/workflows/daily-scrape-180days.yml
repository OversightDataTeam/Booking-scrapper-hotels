name: Daily 180 Days Scrape

on:
  schedule:
    - cron: '0 6 * * *'  # Exécution tous les jours à 6h du matin
  workflow_dispatch:  # Permet l'exécution manuelle

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Setup Node.js
      uses: actions/setup-node@v3
      with:
        node-version: '18'
        
    - name: Install dependencies
      run: npm install
        
    - name: Install Chrome
      run: |
        sudo apt-get update
        sudo apt-get install -y chromium-browser
        
    - name: Create credentials file
      run: |
        echo '${{ secrets.BIGQUERY_CREDENTIALS }}' | sed 's/%$//' | jq '.' > credentials.json
        echo "Credentials file created:"
        cat credentials.json
      env:
        BIGQUERY_CREDENTIALS: ${{ secrets.BIGQUERY_CREDENTIALS }}
        
    - name: Run scraper2.js
      run: node scraper2.js
      env:
        GOOGLE_APPLICATION_CREDENTIALS: credentials.json
        
    - name: Configure Git
      run: |
        git config --global user.name 'GitHub Actions'
        git config --global user.email 'actions@github.com'
        
    - name: Commit and push if changes
      run: |
        git add .
        git diff --quiet && git diff --staged --quiet || (git commit -m "chore: Update data from scraper2.js" && git push) 