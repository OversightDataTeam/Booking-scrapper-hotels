name: Daily Paris Hotel Data Scraper (180 days)

on:
  schedule:
    - cron: '0 2 * * *'  # Tous les jours à 2h00 (après le premier scraper)
  workflow_dispatch:  # Permet de lancer manuellement

env:
  PROJECT_ID: oversight-datalake

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 480  # 8 heures de timeout
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '20'

    - name: Cache dependencies
      uses: actions/cache@v3
      with:
        path: |
          scrapers/hotel-data-180days/node_modules
          ~/.cache/puppeteer
        key: ${{ runner.os }}-node-${{ hashFiles('scrapers/hotel-data-180days/package-lock.json') }}
        restore-keys: |
          ${{ runner.os }}-node-

    - name: Install dependencies
      run: |
        cd scrapers/hotel-data-180days
        npm ci --only=production

    - name: Setup BigQuery credentials and run scraper
      run: |
        cd scrapers/hotel-data-180days
        echo '${{ secrets.GCP_SA_KEY }}' > bigquery-credentials.json
        node scraper.js
